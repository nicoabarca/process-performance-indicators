"""Summary generation from indicator execution results."""

from pathlib import Path

import pandas as pd


def _identify_relevant_columns(formatted_event_log: pd.DataFrame) -> set[str]:
    """Identify columns in the event log that are used by indicators."""
    # Define known column patterns that indicators may use
    known_columns = {
        # Core columns
        "case:concept:name",
        "time:timestamp",
        "lifecycle:transition",
        # Resource columns
        "org:resource",
        "org:role",
        "org:group",
        # Cost columns
        "cost:total",
        "cost:fixed",
        "cost:variable",
        "cost:labor",
        "cost:inventory",
        "cost:maintenance",
        "cost:transportation",
        "cost:warehousing",
        "cost:missed_deadline",
        # Quality columns
        "quality:conformance",
        "quality:accuracy",
        # Custom columns
        "instance_id",
        "outcome:unit",
    }

    # Find which known columns exist in the event log
    existing_columns = set(formatted_event_log.columns)
    relevant = known_columns.intersection(existing_columns)

    # Also check for any quality:* columns that might not be in our predefined list
    for col in existing_columns:
        if col.startswith(("quality:", "cost:")):
            relevant.add(col)

    return relevant


def _count_by_dimension(results_df: pd.DataFrame, dimension: str) -> tuple[int, int]:
    """Count successful and total indicators for a given dimension."""
    dimension_df = results_df[results_df["dimension"] == dimension]
    success_count = len(dimension_df[dimension_df["status"] == "success"])
    total_count = len(dimension_df)
    return success_count, total_count


def _count_by_granularity(results_df: pd.DataFrame, granularity: str) -> tuple[int, int]:
    """Count successful and total indicators for a given granularity."""
    granularity_df = results_df[results_df["granularity"] == granularity]
    success_count = len(granularity_df[granularity_df["status"] == "success"])
    total_count = len(granularity_df)
    return success_count, total_count


def summary_to_csv(
    results_csv_path: str,
    output_csv_path: str,
    formatted_event_log_path: str,
) -> pd.DataFrame:
    """
    Generate a summary CSV from indicator execution results.

    Args:
        results_csv_path: Path to the results CSV file generated by run_indicators_to_csv
        output_csv_path: Path where the summary CSV will be saved
        formatted_event_log_path: Path to the formatted event log CSV

    Returns:
        DataFrame containing the summary statistics

    """
    # Read the results and event log
    results_df = pd.read_csv(results_csv_path)
    event_log_df = pd.read_csv(formatted_event_log_path)

    # Derive event log name from results CSV filename
    # e.g., "production_results.csv" -> "production"
    results_filename = Path(results_csv_path).name
    if results_filename.endswith("_results.csv"):
        event_log_name = results_filename[:-12]  # Remove "_results.csv"
    elif results_filename.endswith(".csv"):
        event_log_name = results_filename[:-4]  # Remove ".csv"
    else:
        event_log_name = results_filename

    # Identify relevant columns
    relevant_cols = _identify_relevant_columns(event_log_df)
    total_cols = len(event_log_df.columns)
    relevant_count = len(relevant_cols)

    # Calculate dimension statistics
    dimensions = ["general", "time", "cost", "quality", "flexibility"]
    dimension_stats = {}
    for dim in dimensions:
        success, total = _count_by_dimension(results_df, dim)
        percentage = (success / total * 100) if total > 0 else 0.0
        dimension_stats[f"{dim}_dimension"] = f"{success} / {total} ({percentage:.1f}%)"

    # Calculate granularity statistics
    granularity_mapping = {
        "instances": "activity_instance_granularity",
        "activities": "activity_granularity",
        "cases": "case_granularity",
        "groups": "group_of_cases_granularity",
    }
    granularity_stats = {}
    for gran, col_name in granularity_mapping.items():
        success, total = _count_by_granularity(results_df, gran)
        percentage = (success / total * 100) if total > 0 else 0.0
        granularity_stats[col_name] = f"{success} / {total} ({percentage:.1f}%)"

    # Calculate overall statistics
    total_success = len(results_df[results_df["status"] == "success"])
    total_indicators = len(results_df)
    overall_percentage = (total_success / total_indicators * 100) if total_indicators > 0 else 0.0

    # Build the summary row
    summary_row = {
        "event_log": event_log_name,
        "relevant_attributes": f"{relevant_count} / {total_cols}",
        **dimension_stats,
        **granularity_stats,
        "overall": f"{total_success} / {total_indicators} ({overall_percentage:.1f}%)",
    }

    # Create DataFrame and save to CSV
    summary_df = pd.DataFrame([summary_row])
    summary_df.to_csv(output_csv_path, index=False)

    return summary_df
